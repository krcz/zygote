# Zygote Project

Zygote is a personal exploration project aimed at possible improvements in how software is developed. For now, the repository contains only goals write-up, but some code should follow soon.

There are 5 areas of focus that are described in this document:

1. **Idiomatic translation** - designing language that can be translated into arbitrary one, in a way where the result code would look like written by a programmer experienced in it

2. **Gradual refinement types/contracts** - expressing function preconditions, postconditions, and invariants in a gradual way, proving some and unit testing the rest

3. **Restricted computation models / robust effect systems** - controlling the expressive power of subroutines to make reasoning about them easier and infer their properties

4. **User-defined substructure in types** - controlling the flow of values in the program, asserting that some operation is eventually done of them

5. **Restricted homoiconic syntax** - treating code as typed data, being able to know that macro application will return correct construct

## 1. Idiomatic translation

When we think about program code two possible ways of making use of it come to mind: **interpretation** and **compilation**. Wikipedia - after *Compilers: Principles, Techniques, and Tools* book - defines compilation as any process of translation between programming languages. Generally though, it is understood as a translation from higher-level programing language (as Haskell, Java, or even C) to a lower-level programming language (as assembler or machine code). It is not always the case: there are decompilers, that attempt to translate lower-level code back into higher-level, attempting to reconstruct as many abstractions as possible, and **transpilers** that translate from one high-level language into another. We are going to focus on the last group.

When are transpilers useful? There are a few possible situations:

* when the platform we want to target supports only one programming language which we don't like - e.g. JavaScript used to be the only language supported by web browsers and still is the only language that can interact with DOM,

* when we want to use a new programming language, but we have existing codebase we have to interact with,

* when we are working in a domain with existing libraries or whole ecosystem written in a specific programming language that we want to interact with,

* when we want to create a new programming language and piggyback the large variety of architectures that C supports - it is much less of a reason now when we have LLVM, but still valid for lots of embedded architectures.

There are two opposing approaches when it comes to transpilation. The first often appears in languages constructed to be transpiled into another, so that most constructs have one to one correspondence between these. Sometimes it is to create a **syntax sugar** (e.g. CoffeScript), sometimes the goal is to add some kind of **verification layer** (e.g. TypeScript). The other approach shows up when basing on existing language with its own existing semantics which is often very different from the one of the target language. In such case transpilers compile the code into low level, assembler-like instructions with some glue code to be able to interact with the target language code. Examples here are CSJS or Scala.js.

Unfortunately, the code generated by the latter category is meant only to be understood by a computer, being completely unreadable to humans. This has several downsides:

* It makes debugging much harder. There are source maps, but error messages often are related to concepts that don't exist in the translated language, only in the target one.

* Interaction between the source and target language is hard, as it requires conversion of data types and using restricted semantics.

* Even if a programmer is familiar with the target language, they cannot see how abstract concepts in the source are mapped - so they can't tell if there is some obvious performance loss or the possibility of corner-case bugs.

* When the translated language is no longer developed, all the code is almost useless and has to be rewritten. No manual improvements or maintenance is possible.

Producing code that is readable, or even more preferably idiomatic is a much better option.  I believe that it doesn't limit us to the category of syntax sugar for a single language though: what if a language was designed to be **flexible** enough to be translated into an **arbitrary target language**? That would definitely be hard, as it would require translation of idioms from one language into another, e.g. translating chain of functional `map` and `filter` calls into an imperative loop. The ultimate goal is to create such translation that would generate code similar to one written by a programmer fluent in the target language and that is what I mean by **idiomatic translation**.

### Fast individual adoption

Embracement of new programming languages is very often limited by the lack of documentation, tutorials, and small motivation to learn it. After all the only place people can use it is their own **hobby projects** - the language is not mature enough to have large companies to trust it and it is hard to find people knowledgable in it. As a small number of programmers is using the language - and mostly in their evenings - not much of text is written on it. That is a **self-perpetuating** mechanism, strong network effect - even if the new language is better it's going to take many years before it becomes popular, if ever.  Most code is written in simple, suboptimal languages like Java or C#.

With traditional programming languages, it is very hard to escape that deadlock. Passionate programmers use technologies they find interesting in their free time while working with old, mundane technologies - that are slowing them down - in their workplace. On the other hand, given the ability to generate clean, idiomatic code in the language their employers expect them to create, people would be able to utilize the new language in their daily work. After all, no one tells them which editor / IDE to use. Tools don't matter, the outcome does.

### Automatic parsing target language libraries and exposing API

Representing rules of the target language - while complex - has limited scope. On the other hand, the number of libraries in the target language ecosystem is constantly growing. Creating wrappers for these requires much boring work. Sometimes it is possible to just use some kind of Foreign Function Interface (FFI), **manually** keeping track of compatibility of types. If there are errors, they often won't be noticed during the translation process, only when compiling the target code. Moreover, **code completion** wouldn't be able to provide suggestions for such calls.

One possible solution here is using the existing information on libraries API (e.g. C header files) and generating corresponding interface definitions for the transpiled language. These could be used as a base for better quality wrappers in the future.

### Working with deprecated technologies

Some languages were heavily used in the past and lost their traction, leaving large codebases and little people qualified to work with them. Nowadays COBOL is a well-known example, in the future others might take the same place (Objective C, Erlang, Matlab). In some cases, there are newer languages able to interact with them (Swift, Elixir), but modifying build processes just to fix a bug or add small functionality is definitely not be the best choice. Being able to generate code would allow **modifying old codebases** without writing outdated syntax.

### Learning new programming languages

All programming languages have their idioms and it's often hard to learn these: one uses more verbose constructions, not knowing that there are other possibilities. The idiomatic translation would generate these, allowing the user to become familiar with them.

## 2. Refinement types / contracts

Types in programming represent **sets of possible values** and **allowed operations** on them. In languages with **dynamic typing** values generally need to be tagged with their type information during runtime, so that the interpreter can determine if operations on it are legal and what do they mean semantically. In languages with **static typing** variables and expressions are tagged with type info, constraining possible values. That means that all operations can be resolved during compilation and information about types is no longer necessary (it is often still kept to provide RTTI and verify downcasting).

In most programming languages values have a type assigned at the moment of construction (let's call it essential type). There is a hierarchy of types, partially ordered set with "is **subtype of**" relation. If the essential type of value is A, A is subtype of B, then the value is also of type B. In such programming languages, the set of types it is compatible with is fully determined by its essential type.

There are programming languages though where the typing system is much more robust and allows representation of more subtle constraints. Example I find interesting are **refinement types** which enrich simple type with formula value satisfies, e.g. in F\* type `x : T { p(x) }` means that for value `v (x : T { p(x) })` (or using syntax sugar `v: T { p(v) }`) we have `p(v)`. Subtyping relation (for the same simple type) can be viewed as given by logical relations between predicates: `x : T { q(x) }` is subtype of `x : T { p(x) }` if and only if `\forall v: p(v) -> q(v)`.

Refinement types allow expressing complex assertions about variables and **verifying** them during compilation time.

### Proven vs tested assertions

Proving the correctness of programs is a demanding process. It requires not only analysing all possible execution paths of the program but also encoding the results of such analysis in a form that automatic prover can understand. At the same in most codebases, not all parts are equally important and the consequences of bugs in various parts are totally different. Error in the storage layer might cause loss or corruption of a large amount of data. Error in core algorithm might produce wrong results but probably won't affect other results. An error in user interface code might be just a nuisance. Moreover, the most crucial parts of the codebase can often be reduced to the essence, solid and predictable, while business logic and user interface - parts that have to deal with complexities of the real world and multitudes of libraries - can be kept separated, using core interface. Bugs in the outer layer don't affect the inner layer.

Basing on that I think that it would be useful to have some kind of **gradual verification** system if we ever want formal methods in programming to become mainstream. When we embed predicates into the code, we can prove some of them, and treat others as unproven contracts. Program correctness verification would be based on such assumptions and it would be possible to make compiler list these and possibly reduce the number by adding new proofs.

But there exists another method of verifying correctness, which is not sound, but it is much simpler: **testing**. Creating test suites can be often tedious and hard to organize. It could be replaced by adding contracts to the codebase and then running it with example data to check if the contracts hold.

### Automatic conversion

Refinement types often mean that value checks are moved from the callee side to the caller: when `sqrt` function requires the argument to be non-negative, the user has to convert it to the corresponding refined type. That might mean introducing visual noise, but there is a solution for that. Having value `v : T` language could automatically convert it to `v : T { p(v) }` by adding an assertion before the call. Such assertion can fail, giving rise to behaviour depending on kinds of error handling available: e.g. exceptions in Java or Rust `Result` type.

### Efficient boundary checks

Checking bounds of array checks is most often done during runtime - `get(a: array, i: int)` verifies that `i` is valid. If we had instead function `get(a: array, i: int {1 <= i <= len(a)})` the check could be avoided, still achieving security of the algorithm. An example where it would do much is Floyd-Warshall algorithm:
```
for k in 1 .. n:
  for i in 1 .. n:
    for j in 1 .. n:
      C[i][j] = min(C[i][j], A[i][k] + B[k][j])
```

Using refinement types save us 8 bound checks overhead for just 2 operations and 1 assignment.

## 3. Restricted computation models / robust effect systems

Just as refinement types tell us what values can appear in the program, we can also model **how these values are computed**. The more powerful constructions code uses, the harder it is to analyse. Immutability is simpler than local mutability, which is simpler than a mutating global state. Arithmetic expressions are simpler than no-loops no-recursion code, which is simpler than Turing complete constructions. Total functions are simpler than exception throwing / possibly infinite ones. The actor model is simpler than full concurrency with locks and conditions.

With great power comes great responsibility and limiting your power, adding some rules to it, can make dealing with it easier. The best code I've seen tries to write as much as possible using simple constructions, constraining most complexity in only a small part of the codebase. This makes it easier to understand and see possible errors and obstacles. Unfortunately, small mutations or nondeterministic behaviour might appear in functions used, as sometimes it is hard to notice.

If one could choose the computation model per specific methods, it could be checked by the compiler, that it doesn't use any more complex subroutines. Function declaration would guarantee its behavior.

### Actor model

Let's think of two widely used implementations of the actor model. First one is Erlang with its virtual machine. As it is a separate programming language, it can assure that all messages are immutable and the state of an actor is modified only in the message processing loop. The second implementation is Akka, which runs on JVM. It has no way of asserting values immutability and nothing protects mutable actor state from being modified by other threads, which I've seen many times to create hard to debug race conditions.

Being able to restrict state access to functions called by the message processing loop and requiring the messages to be immutable, we could write safe, actor model-based code even in languages that allow using mutable constructions.

### Computational complexity verification

It might be possible to create a system that allowed tagging functions with their **computational complexity** - how execution time depends on its arguments. The easier bound is pessimistic time, but expressive enough model should make average case or even amortized cost analysis possible. That way we could create libraries with provable efficiency, knowing that even corner cases won't make it sluggish.

## 4. User-defined substructure in types

A **substructural type system** is a type system that controls how many times a variable (or expression result) is used. The most common class are **linear type systems**, where each variable must be used exactly once. That allows safe memory management without garbage collection - type system assures that each initialized variable is eventually destroyed. Other kinds of subtyping exist as well: **affine types**, where the variable can be used at most once and **relevant types**, where the variable must be used at least once.

Examples of languages with linear types are Rust, Idris, F\*. Linearity is baked deep into them. Would it be possible to define substructure per variable?

### Guaranteed exception logging

There are few approaches on where to log errors/warnings related to exception:

* When throwing the exception: this has upside of being sure the error is logged, but **less context** information is available - let's use collection code as an example; it doesn't need to know what kind of data is stored there and is accessed

* In code catching the exception: more information is available, but easy to forget about it in some cases - especially in languages without checked exceptions or monadic error type. But even there - it is easy to ignore the error, e.g. in Scala `Try[Unit]` or Rust `Result<SomeError, Unit>`

* Both places: this leads to redundant logs

When mixing both styles it's even easier to forget about logging. With relevant types (i.e. the variable must be used at least once) it would be possible to assert that each exception is logged, requiring either logging it or propagating it upward.

## 5. Restricted homoiconic syntax

**Homoiconicity** - treating the code just as a simple data structure - allows easy **extensibility** of programming languages. In LISP everything is a list - and as elements of such lists can be lists themselves, it allows a natural representation of trees. This works great for dynamically typed languages which the most LISP dialects are.

Extensibility doesn't come without a cost. When it's too easy every programmer creates their own abstractions, it's sometimes easier to create a new one that extends the existing to your needs. That means that there are lots of libraries for a single purpose, but none of them is heavily tested or documented. Such an effect is so pervasive it even got its name: [**the LISP curse**](http://winestockwebdesign.com/Essays/Lisp_Curse.html).

The other side of the spectrum is having languages with complex, written-in-stone **grammars**, which leads to **typed ASTs**. That has an upside of clear structure, making it easier to understand the code. On the other side, such syntax doesn't have flexibility: most often there are **no macros** and if they exist, they are much more complex than in LISP. That ends in using suboptimal solutions, such as preprocessor in C, annotations + code generation in Java ([Lombok](https://projectlombok.org/)), or even text-based code generation [some people use in Go](https://www.calhoun.io/using-code-generation-to-survive-without-generics-in-go/).

I would love to see something in-between: the ability to create simple grammars for DSL without much effort, but at the same time be required to define their schema in a simple manner. My current idea here is having LISP-like lists (s-expressions) such that elements in lists would follow rules defined by **regular expressions**, but not regular expressions on characters, but on atoms (identifiers and constants). Regular languages allow efficient parsing and are easy to understand. Compilers would be able to map its state to a position in the regular expression and explain what is expected next.

### New configuration & data format

Just as there are many programming languages, there are many configuration formats, each with their own syntax, own string interpolation rules (or lack of). One could have a common one and idiomatically translate into the other as well. This also provides the possibility of validating it during translation, instead of doing it in runtime.

### Safe syntactic macros / transducers

When expressions structure is simple enough, it is possible to define **transformations** in the domain of such expressions. Such action would be formally proven to always generate correct expressions. We can consider [Finite-state transducers](https://en.wikipedia.org/wiki/Finite-state_transducer) an example - they allow translation between regular languages. It should be possible to define more advanced transformation systems - still restricted to be total functions that given input following the input schema generate proper output.

